{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23746603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff96ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"News-Classification-Dataset/barely-true-cleaned.csv\")\n",
    "df2 = pd.read_csv(\"News-Classification-Dataset/false-cleaned.csv\")\n",
    "df3 = pd.read_csv(\"News-Classification-Dataset/half-true-cleaned.csv\")\n",
    "df4 = pd.read_csv(\"News-Classification-Dataset/mostly-true-cleaned.csv\")\n",
    "df5 = pd.read_csv(\"News-Classification-Dataset/pants-fire-cleaned.csv\")\n",
    "df6 = pd.read_csv(\"News-Classification-Dataset/true-cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837d6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0e810",
   "metadata": {},
   "source": [
    "## Checking the data for imbalances\n",
    "\n",
    "- Class imbalances can be a serious problem for machine learning models.\n",
    "\n",
    "If there are class imbalances, the model may not learn to predict the minority class well. This can be solves using \n",
    "- oversampling\n",
    "- Weighted classes\n",
    "- SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6db49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (15000, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Statement  15000 non-null  object\n",
      " 1   Label      15000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Basic overview of the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb9b73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_values[missing_values > 0])  # Only show columns with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d08826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class count (%):\n",
      "Label\n",
      "barelytrue    2500\n",
      "False         2500\n",
      "halftrue      2500\n",
      "mostlytrue    2500\n",
      "pantsfire     2500\n",
      "True          2500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution \n",
    "class_count = df['Label'].value_counts()\n",
    "print(\"\\nClass count (%):\")\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9625f9",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- Padding and Truncation are very crucial for the model to work properly.\n",
    "- Padding is used to make all sequences the same length.\n",
    "- Truncation is used to cut off sequences that are too long.\n",
    "- The tokenizer will automatically pad and truncate the sequences to the maximum length of the model.\n",
    "- The tokenizer will also convert the text to input IDs and attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8acd6f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text:\n",
      "Every study has shown that when work requirements are tied to federal safetynet programs it puts more people to work.\n",
      "\n",
      "Tokenized sample text:\n",
      "{'input_ids': [101, 2296, 2817, 2038, 3491, 2008, 2043, 2147, 5918, 2024, 5079, 2000, 2976, 3808, 7159, 3454, 2009, 8509, 2062, 2111, 2000, 2147, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "## Sample text and tokenization\n",
    "sample_text = df['Statement'].iloc[0]\n",
    "print(\"\\nSample text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nTokenized sample text:\")\n",
    "sample_tokens = tokenizer(sample_text, truncation=True, padding='max_length', max_length=128)\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0219b8",
   "metadata": {},
   "source": [
    "## Model\n",
    "- The model is a standard BERT model that is pre-trained.\n",
    "\n",
    "Choosing the right model is very crucial\n",
    "- distilbert is used for systems for low latency and high throughput.\n",
    "- bert-base-uncased is used for systems that require high accuracy and can afford to run slower.\n",
    "- bert-large-uncased is used for systems that require very high accuracy and can afford to run slower.\n",
    "\n",
    "The performace difference between the models depends on the task and the dataset. But in general, the larger the model, the better the performance.\n",
    "- But distilbert usually performs within a few percentage points of bert-base-uncased and is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae547460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd599e",
   "metadata": {},
   "source": [
    "## Label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edf6fcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label to ID mapping: {'barelytrue': 0, False: 1, 'halftrue': 2, 'mostlytrue': 3, 'pantsfire': 4, True: 5}\n",
      "\n",
      "ID to Label mapping: {0: 'barelytrue', 1: False, 2: 'halftrue', 3: 'mostlytrue', 4: 'pantsfire', 5: True}\n"
     ]
    }
   ],
   "source": [
    "label_to_id = {label: id for id, label in enumerate(df['Label'].unique())}\n",
    "print(\"\\nLabel to ID mapping: \", end=\"\")\n",
    "print(label_to_id)\n",
    "\n",
    "id_to_label = {id: label for label, id in label_to_id.items()}\n",
    "print(\"\\nID to Label mapping: \", end=\"\")\n",
    "print(id_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797f8c1",
   "metadata": {},
   "source": [
    "## Freezing layers\n",
    "\n",
    "- Freezing layers is a technique used to prevent the model from updating the weights of certain layers during training.\n",
    "- This is useful when you want to fine-tune a pre-trained model on a new task.\n",
    "- Freezing layers can help to prevent overfitting and speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7dc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fff5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fb109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde0142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca83e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620ed5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
